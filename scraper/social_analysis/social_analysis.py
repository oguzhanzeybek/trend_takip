import os
import json
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
from pathlib import Path
import time
import sys
import re
import datetime
import math 

# --- AYARLAR ---
# En stabil, en hÄ±zlÄ± ve maliyeti en dÃ¼ÅŸÃ¼k model: GPT-4o-mini
MODEL_NAME = "openai/gpt-4o-mini" 

# Stabil ve hÄ±zlÄ± iÅŸlem iÃ§in ideal ayarlar
BATCH_SIZE = 50 # Her bir AI isteÄŸi iÃ§in 50 satÄ±r veri
WAIT_TIME = 1 # 1 saniye dinlenme

# --- BAÄLANTI ---
BASE_DIR = Path(__file__).resolve().parent

# .env dosyasÄ±nÄ± bulma ve yÃ¼kleme
env_path = None
search_dirs = [BASE_DIR] + list(BASE_DIR.parents)[:3]
for d in search_dirs:
    if (d / '.env').exists():
        env_path = d / '.env'
        load_dotenv(dotenv_path=env_path)
        break

api_key = os.getenv("OPENROUTER_KEY")
if not api_key:
    print("âŒ HATA: OPENROUTER_KEY bulunamadÄ±!")
    sys.exit(1)

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=api_key,
)

# --- YARDIMCI FONKSÄ°YONLAR ---

def truncate_text(text, max_chars=1000):
    """Token maliyetini dÃ¼ÅŸÃ¼rmek iÃ§in metni kÄ±saltÄ±r."""
    if len(text) > max_chars:
        return text[:max_chars] + "..."
    return text

def clean_data(df):
    """
    TEMÄ°ZLÄ°K VE KIRPMA
    """
    initial_len = len(df)
    print(f"  ğŸ§¹ Ã–n temizlik... (GiriÅŸ: {initial_len})")
    
    # SADECE tamamen boÅŸ satÄ±rlarÄ± ve duplike satÄ±rlarÄ± atar
    df = df.dropna(how='all').drop_duplicates() 
    
    df_temp = df.copy()
    if df_temp.shape[1] > 1:
        # Ä°ndeks 1'den (ikinci sÃ¼tun) sonrasÄ± kÄ±rpÄ±lÄ±r.
        df_temp.iloc[:, 1:] = df_temp.iloc[:, 1:].astype(str).apply(
            lambda col: col.apply(lambda x: truncate_text(x, 1000))
        )
    
    print(f"  âœ¨ Veri HazÄ±r: {len(df_temp)} satÄ±r")
    return df_temp.astype(str) 

def get_output_file_path(filename):
    # Data klasÃ¶rÃ¼ yoksa oluÅŸtur
    output_dir = BASE_DIR / "data"
    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir / f"analyzed_{filename}.json"

def save_analysis_json(data, filename):
    output_path = get_output_file_path(filename)
    
    # JSON dosyasÄ±nÄ± oluÅŸturup kaydet
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"  ğŸ’¾ Analiz Sonucu Kaydedildi: {output_path.name}")


# ğŸš¨ GÃœNCEL VE ULTRA DETAYLI ANALÄ°Z FONKSÄ°YONU ğŸš¨
def analyze_data_with_ai(data_chunk, df_columns, is_final_analysis=False, retry=0):
    column_names = ", ".join(df_columns) 
    current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # ROL TANIMI: ARTIK "GENEL UZMAN" DEÄÄ°L, "VERÄ° DEDEKTÄ°FÄ°"
    role = "**Sen TÃ¼rkiye'nin en obsesif Veri Madencisi ve Sosyal Medya Dedektifisin. Senin iÅŸin genellemeler yapmak deÄŸil, Ã¶nÃ¼ne gelen veri parÃ§asÄ±ndaki (batch) benzersiz ve spesifik parmak izlerini bulmaktÄ±r. Asla varsayÄ±mlarla konuÅŸmazsÄ±n, sadece kanÄ±tla konuÅŸursun.**"
    
    if is_final_analysis:
        # FÄ°NAL ANALÄ°Z PROMPT'u
        prompt_goal = "GÃ¶revin, saÄŸlanan TÃœM ara analiz Ã¶zetlerini (batch sonuÃ§larÄ±nÄ±) birleÅŸtirerek, tekrar eden kalÄ±plarÄ± deÄŸil, verilerin toplamÄ±ndan Ã§Ä±kan BÃœYÃœK RESMÄ°, Ã§eliÅŸkileri ve nÃ¼anslarÄ± raporlamaktÄ±r. **Ezbere cÃ¼mleler kurma, analiz edilen binlerce satÄ±rÄ±n gerÃ§ek hikayesini anlat.**"
        data_header = "VERÄ° (Toplu iÅŸlerden gelen ara analiz Ã¶zetleri):"
        analysis_structure = """
    1. **Ana Duygu Durumu:** TÃ¼m parÃ§alara baktÄ±ÄŸÄ±nda halkÄ±n gerÃ§ek ruh hali nedir? (Sadece 'endiÅŸe' deyip geÃ§me; Ã¶fke mi, bÄ±kkÄ±nlÄ±k mÄ±, alaycÄ± bir neÅŸe mi? DetaylandÄ±r).
    2. **BaskÄ±n GÃ¼ndemler:** Verilerde en Ã§ok tekrar eden 3 somut olay/konu nedir?
    3. **Harcama EÄŸilimi:** Ä°nsanlar neyden ÅŸikayet ediyor veya neye para harcÄ±yor? SektÃ¶rel bazda (GÄ±da, Giyim, Teknoloji vb.) Ã§Ä±karÄ±m yap.
    4. **Gelecek Tahmini:** Bu verilere dayanarak Ã¶nÃ¼mÃ¼zdeki 3 ayda ne olmasÄ± muhtemel?
    5. Ã‡IKTI sadece ve sadece tek bir JSON nesnesi olmalÄ±dÄ±r.
        """
        json_output_template = f"""
    "analiz_tarihi": "{current_time}",
    "analiz_kaynaÄŸÄ±": "social_media.csv",
    "genel_deÄŸerlendirme": "Verilerin tamamÄ±na dayalÄ±, genellemelerden uzak, Ã§ok katmanlÄ± ve derinlemesine bir Ã¶zet paragraf.",
    "ana_duygular": [
      {{ "duygu": "Duygu AdÄ± 1", "skor": 0-100, "gerekÃ§e": "Bu duygunun kaynaÄŸÄ± olan spesifik olaylar ve veriler." }},
      {{ "duygu": "Duygu AdÄ± 2", "skor": 0-100, "gerekÃ§e": "Bu duygunun kaynaÄŸÄ± olan spesifik olaylar ve veriler." }}
    ],
    "baskin_gundemler": [
      {{ "konu": "Konu BaÅŸlÄ±ÄŸÄ± 1", "kÃ¶ken": "Bu konuyu tetikleyen sosyal medya iÃ§erikleri." }}, 
      {{ "konu": "Konu BaÅŸlÄ±ÄŸÄ± 2", "kÃ¶ken": "Bu konuyu tetikleyen sosyal medya iÃ§erikleri." }}
    ],
    "harcama_egilimi_analizi": {{
        "egilim": "TÃ¼ketici davranÄ±ÅŸÄ±ndaki net deÄŸiÅŸim.",
        "sektor_etkisi": "Etkilenen sektÃ¶rler ve nedenleri."
    }},
    "gelecek_tahminleri": [
        {{ "tahmin": "Tahmin 1", "risk_seviyesi": "YÃ¼ksek/Orta/DÃ¼ÅŸÃ¼k", "neden": "Dayanak noktasÄ±." }},
        {{ "tahmin": "Tahmin 2", "risk_seviyesi": "YÃ¼ksek/Orta/DÃ¼ÅŸÃ¼k", "neden": "Dayanak noktasÄ±." }}
    ]
        """
    else:
        # ARA ANALÄ°Z (BATCH) PROMPT'u: BURASI Ã‡OK KRÄ°TÄ°K DEÄÄ°ÅTÄ°RÄ°LDÄ°
        # Modelin kopya Ã§ekmesini engellemek iÃ§in "Ã¶rnek iÃ§erikleri" kaldÄ±rdÄ±k.
        prompt_goal = "GÃ¶revin, sana verilen **bu spesifik 50 satÄ±rlÄ±k veri parÃ§asÄ±nÄ±** incelemektir. **DÄ°KKAT: Asla Ã¶nceki bildiklerini veya genel geÃ§er 'ekonomi kÃ¶tÃ¼' ezberlerini kullanma.** Sadece bu metinlerde geÃ§en **Ã–ZEL Ä°SÄ°MLERÄ°, MARKALARI, OLAYLARI ve HASHTAG'LERÄ°** raporla. EÄŸer metinlerde futbol varsa futbol yaz, dizi varsa dizi yaz. Veri ne diyorsa o.SEN BÄ°R TOPLUM BÄ°LÄ°MCÄ°SÄ° BÄ°R DAHÄ°SÄ°N , Ä°NSANLIÄIN KURTARICI OLARAK TANRI GIBI KUÅBAKIÅI ANALÄ°Z ET KÄ° HALKI ANLAYABÄ°LELÄ°M."
        data_header = f"VERÄ° (Bu Batch Ä°Ã§in Ham Metinler): {data_chunk}"
        analysis_structure = """
    1. **Ã–zet Duygu:** SADECE BU 50 satÄ±rda hissedilen en baskÄ±n duygu.
    2. **Duygu GerekÃ§esi:** Neden bu duygu? Metinlerin iÃ§inden **spesifik Ã¶rnekler** vererek aÃ§Ä±kla. (Ã–rn: 'X kullanÄ±cÄ±sÄ± Y olayÄ±na kÄ±zdÄ±ÄŸÄ± iÃ§in' gibi).
    3. **Ã–zet Konu:** Bu grupta insanlar tam olarak neyden bahsediyor? (Genel 'hayat' deme. 'Zam gelen sÃ¼t fiyatÄ±' de, 'X dizisindeki karakter' de).
    4. **Konu GerekÃ§esi:** Bu konuyu kanÄ±tlayan **anahtar kelimeleri** yaz.
    5. Ã‡IKTI sadece ve sadece tek bir JSON nesnesi olmalÄ±dÄ±r.
        """
        # Åablondaki Ã¶rnek deÄŸerleri sildim ki model onlarÄ± kopyalamasÄ±n!
        json_output_template = """
      "ozet_duygu": "BURAYA_BU_VERÄ°DEKÄ°_BASKIN_DUYGUYU_YAZ ve detaylÄ± acÄ±klama yap",
      "duygu_gerekcesi": "BURAYA_METÄ°NLERDEN_KANIT_VE_ALINTI_Ä°Ã‡EREN_GEREKÃ‡EYÄ°_YAZ ve acÄ±klama yap",
      "ozet_konu": "BURAYA_BU_VERÄ°DEKÄ°_SPESÄ°FÄ°K_KONUYU_YAZ ve acÄ±klama yap",
      "konu_gerekcesi": "BURAYA_KONUYU_DESTEKLEYEN_ANAHTAR_KELÄ°MELERÄ°_YAZ ve acÄ±klama yap"
        """


    # PROMPT YapÄ±sÄ±
    prompt = f"""
    SEN KRÄ°TÄ°K BÄ°R ROLÃœ ÃœSTLENÄ°YORSUN. SADECE Ä°STENEN JSON Ã‡IKTISINI ÃœRET. BAÅKA HÄ°Ã‡BÄ°R AÃ‡IKLAMA VEYA GÄ°RÄ°Å METNÄ° KULLANMA.
    
    Sen, {role}
    {prompt_goal}
    
    AMACIN: Verideki gÃ¼rÃ¼ltÃ¼yÃ¼ deÄŸil, sinyali yakalamaktÄ±r.HalkÄ±n nabzÄ±nÄ± tutarak bir toplum bilimci gibi derinlemesine analiz yap.
    
    GÃ–REV: AÅŸaÄŸÄ±daki sosyal medya verilerini **{ 'BÃœTÃœNSEL' if is_final_analysis else 'ARA (BATCH)' }** olarak analiz et.
    Kolon Ä°simleri (SÄ±rayla): [{column_names}]
    
    {analysis_structure}
    
    {data_header}
    
    Ã‡IKTI: {{
      {json_output_template}
    }}
    """
    
    try:
        if retry == 0:
            print(f"    ğŸ’¬ AI Analizi BaÅŸlatÄ±lÄ±yor... ({'Nihai Ã‡OK DETAYLI Rapor' if is_final_analysis else 'Batch - KapsamlÄ± GerekÃ§eli Ã–zet'})")
        
        completion = client.chat.completions.create(
            extra_headers={"HTTP-Referer": "http://localhost", "X-Title": "SentimentAnalyzer"},
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5, # Maksimum detay ve aÃ§Ä±klama iÃ§in sÄ±caklÄ±k 0.5'e yÃ¼kseltildi
        )
        
        resp = completion.choices[0].message.content
        
        # JSON Temizleme (Mevcut koddan korundu)
        if "```" in resp:
            match = re.search(r"```json\s*(.*?)\s*```", resp, re.DOTALL)
            if match:
                resp = match.group(1).strip()
            else:
                resp = resp.replace("```", "").strip()
        
        resp = re.sub(r'//.*', '', resp) 
        
        return json.loads(resp)
    
    except Exception as e:
        err = str(e)
        if "402" in err or "insufficient_quota" in err:
            print("\nâŒ HATA: Yetersiz Bakiye! LÃ¼tfen OpenRouter'a kredi yÃ¼kleyin.")
            sys.exit(1)
            
        if retry < 2:
            print(f"      âš ï¸ GeÃ§ici Hata ({e.__class__.__name__}). Tekrar deneniyor... ({retry+1})")
            time.sleep(5)
            return analyze_data_with_ai(data_chunk, df_columns, is_final_analysis, retry + 1)
            
        print(f"\nâŒ Kritik Hata. AI'dan analiz alÄ±namadÄ±. Hata: {err}")
        return None

# --- ANA DÃ–NGÃœ (Batch Ä°ÅŸleme MantÄ±ÄŸÄ± Korundu) ---
def process_social_media_analysis():
    # --- DEÄÄ°ÅÄ°KLÄ°K BURADA: Dinamik Dosya Yolu ---
    # Eski sabit yol yerine, scriptin olduÄŸu yerden yola Ã§Ä±karak raw_data'yÄ± buluyoruz.
    raw_data_dir = BASE_DIR.parent / "ai_filter" / "Raw_data"
    
    # Ã‡Ä±ktÄ± klasÃ¶rÃ¼ kontrolÃ¼ (varsa kullan, yoksa oluÅŸtur)
    output_dir = BASE_DIR / "data"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    filename = "social_media.csv"
    
    print(f"ğŸ“‚ Okunacak: {raw_data_dir / filename}")
    print(f"ğŸ’ Model: {MODEL_NAME} (HAZIR)")
    print("------------------------------------------------")

    if not (raw_data_dir / filename).exists():
        print(f"âŒ HATA: {filename} dosyasÄ± bulunamadÄ±. LÃ¼tfen yolu kontrol edin:")
        print(f"   Aranan Yer: {raw_data_dir / filename}")
        return

    print(f"\nğŸš€ {filename} TOPLUMSAL NABIZ ANALÄ°ZÄ° BAÅLIYOR...")
    
    try:
        # TÃ¼m veriyi oku
        df = pd.read_csv(raw_data_dir / filename, dtype=str, low_memory=False).fillna("")
    except Exception as e:
        print(f"âŒ Dosya okuma hatasÄ±: {e}")
        return
    
    # Veriyi temizle ve kÄ±rp
    df_clean = clean_data(df)
    total_rows = len(df_clean)
    
    if total_rows == 0:
        print("âŒ Temizlenecek veri bulunamadÄ±. Analiz yapÄ±lamÄ±yor.")
        return
    
    # --- BATCH Ä°ÅLEME MANTIÄI ---
    
    num_batches = math.ceil(total_rows / BATCH_SIZE)
    intermediate_summaries = []
    df_columns = df_clean.columns.tolist()

    print(f"  ğŸ“ Toplam {total_rows} satÄ±r, {num_batches} toplu iÅŸ (batch) halinde iÅŸlenecek.")
    
    # ParÃ§alarÄ± dÃ¶ngÃ¼de iÅŸleme
    for i in range(num_batches):
        start_index = i * BATCH_SIZE
        end_index = min((i + 1) * BATCH_SIZE, total_rows)
        
        batch_df = df_clean.iloc[start_index:end_index]
        batch_data_chunk = batch_df.to_string(header=False, index=False)
        
        print(f"\n--- Batch {i+1}/{num_batches} (SatÄ±r {start_index} - {end_index-1}) ---")
        
        # Ara analizi yap
        batch_analysis = analyze_data_with_ai(batch_data_chunk, df_columns, is_final_analysis=False)
        
        if batch_analysis:
            # Ara sonuÃ§larÄ± listeye ekle (ArtÄ±k daha detaylÄ± Ã¶zetler alÄ±nÄ±yor)
            summary = (
                f"Batch {i+1} Ã–zeti: Ana Duygu: {batch_analysis.get('detaylÄ±_hissedilen_duygu', 'Bilinmiyor')} "
                f"(GerekÃ§e: {batch_analysis.get('duygu_gerekcesi', 'Yok')}), "
                f"Ana Konu: {batch_analysis.get('ozet_konu', 'Bilinmiyor')} "
                f"(GerekÃ§e: {batch_analysis.get('konu_gerekcesi', 'Yok')})"
            )
            intermediate_summaries.append(summary)
            print(f"  âœ”ï¸ Batch {i+1} TamamlandÄ±. Ã–zet: {summary}")
        else:
            print(f"  âŒ Batch {i+1} Analizi baÅŸarÄ±sÄ±z. AtlanÄ±yor.")

        time.sleep(WAIT_TIME)

    if not intermediate_summaries:
        print("\nâŒ HiÃ§bir batch analiz edilemedi. Nihai analiz yapÄ±lamÄ±yor.")
        return

    # --- NÄ°HAÄ° ANALÄ°Z ---
    final_input_data = "\n".join(intermediate_summaries)
    print("\n================================================")
    print("ğŸ§  ARA ANALÄ°ZLER BÄ°RLEÅTÄ°RÄ°LÄ°YOR: NIHAI Ã‡OK DETAYLI ANALÄ°Z BAÅLIYOR...")
    print("================================================")
    
    # Ara Ã¶zetleri kullanarak nihai bÃ¼tÃ¼nsel analizi yap
    final_analysis_result = analyze_data_with_ai(
        data_chunk=final_input_data, 
        df_columns=["ozet_duygu", "duygu_gerekcesi", "ozet_konu", "konu_gerekcesi"], 
        is_final_analysis=True
    )
    
    if final_analysis_result:
        # Sonucu JSON dosyasÄ±na kaydet
        save_analysis_json(final_analysis_result, filename.split('.')[0] + "_ultra_detailed_sentiment")
        print("\nğŸ‰ TOPLUMSAL NABIZ ANALÄ°ZÄ° BAÅARIYLA TAMAMLANDI!")
    else:
        print("\nâŒ Nihai BÃ¼tÃ¼nsel Analiz baÅŸarÄ±sÄ±z oldu.")


if __name__ == "__main__":
    process_social_media_analysis()