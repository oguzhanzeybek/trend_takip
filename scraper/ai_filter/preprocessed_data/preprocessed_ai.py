import os
import json
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
from pathlib import Path
import time
import sys

# --- AYARLAR (Ã‡ALIÅAN FÄ°NAL MOD) ---
# En stabil, en hÄ±zlÄ± ve maliyeti en dÃ¼ÅŸÃ¼k model: GPT-4o-mini
MODEL_NAME = "openai/gpt-4o-mini"

# Stabil ve hÄ±zlÄ± iÅŸlem iÃ§in ideal ayarlar
BATCH_SIZE = 50 
WAIT_TIME = 1  # 1 saniye dinlenme (HÄ±z iÃ§in)

# --- BAÄLANTI VE DÄ°NAMÄ°K YOL ---
BASE_DIR = Path(__file__).resolve().parent

env_path = None
search_dirs = [BASE_DIR] + list(BASE_DIR.parents)[:3]
for d in search_dirs:
    if (d / '.env').exists():
        env_path = d / '.env'
        load_dotenv(dotenv_path=env_path)
        break

api_key = os.getenv("OPENROUTER_KEY")
if not api_key:
    print("âŒ HATA: OPENROUTER_KEY bulunamadÄ±!")
    sys.exit(1)

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=api_key,
)

# --- YARDIMCI FONKSÄ°YONLAR ---

def truncate_text(text, max_chars=1000):
    """Token maliyetini dÃ¼ÅŸÃ¼rmek iÃ§in metni kÄ±saltÄ±r."""
    if len(text) > max_chars:
        return text[:max_chars] + "..."
    return text

def clean_data(df):
    """
    TEMÄ°ZLÄ°K VE KIRPMA (Kaynak SÃ¼tunu KorumalÄ±)
    âš ï¸ YALNIZCA FORMATLAMA VE TOKEN TASARRUFU YAPAR, SATIR ELEME Ä°ÅLEMÄ° AI'YA DEVREDÄ°LDÄ°.
    """
    initial_len = len(df)
    print(f"   ğŸ§¹ Ã–n temizlik... (GiriÅŸ: {initial_len})")
    
    # SADECE tamamen boÅŸ satÄ±rlarÄ± ve duplike satÄ±rlarÄ± atar
    df = df.dropna(how='all').drop_duplicates() 
    
    # 1. KÄ±rpma Ä°ÅŸlemi (SADECE 2. SÃ¼tundan itibaren)
    df_temp = df.copy()
    if df_temp.shape[1] > 1:
        # Ä°ndeks 1'den (ikinci sÃ¼tun) sonrasÄ± kÄ±rpÄ±lÄ±r.
        df_temp.iloc[:, 1:] = df_temp.iloc[:, 1:].astype(str).apply(
            lambda col: col.apply(lambda x: truncate_text(x, 1000))
        )
    
    print(f"   âœ¨ Veri HazÄ±r (AI Elemesi iÃ§in): {len(df_temp)} satÄ±r")
    return df_temp.astype(str) 

def get_progress_file_path(filename):
    # Progress dosyalarÄ± scriptin olduÄŸu yerdeki 'data' klasÃ¶rÃ¼nde tutulur
    data_dir = BASE_DIR / "data"
    data_dir.mkdir(parents=True, exist_ok=True)
    return data_dir / f"{filename}_progress.txt"

def get_last_index(filename):
    p_file = get_progress_file_path(filename)
    if p_file.exists():
        with open(p_file, "r") as f:
            try: return int(f.read().strip())
            except: return 0
    return 0

def save_progress(filename, index):
    with open(get_progress_file_path(filename), "w") as f:
        f.write(str(index))

def append_to_csv(data, filename):
    # Ã‡Ä±ktÄ± klasÃ¶rÃ¼: scriptin olduÄŸu yerdeki 'data' klasÃ¶rÃ¼
    output_path = BASE_DIR / "data" / f"filtered_{filename}.csv"
    df = pd.DataFrame(data)
    if not output_path.exists():
        df.to_csv(output_path, index=False, encoding='utf-8-sig', mode='w')
    else:
        df.to_csv(output_path, index=False, encoding='utf-8-sig', mode='a', header=False)

def analyze_paid_fast(data_chunk, category, df_columns, retry=0):
    # Kolon isimlerini prompt'a ekliyoruz ki AI neye baktÄ±ÄŸÄ±nÄ± bilsin
    column_names = ", ".join(df_columns) 
    
    # PROMPT (DEÄÄ°ÅTÄ°RÄ°LMEDEN KORUNDU)
    prompt = f"""
    Sen, **Metro Market'in HORECA (Otel, Restoran, Catering) SektÃ¶rÃ¼ne odaklanmÄ±ÅŸ YÃ¼ksek Seviye Stratejik Pazar Analistisin.** Senin gÃ¶revin, sadece Ã¼rÃ¼n seÃ§mek deÄŸil, piyasadaki **YENÄ° BAÅLANGIÃ‡ TRENDLERÄ°NÄ° ERKEN TESPÄ°T ETMEK** ve mÃ¼ÅŸteri ihtiyaÃ§larÄ±na gÃ¶re **pazarda devrim yaratacak Ã¼rÃ¼n portfÃ¶yÃ¼nÃ¼** oluÅŸturmaktÄ±r.
    potansiyel mÃ¼ÅŸterilerin beklentileri, sektÃ¶r trendleri ve yenilikÃ§i Ã¼rÃ¼n Ã¶zellikleri hakkÄ±nda derinlemesine bilgiye sahipsin.
    potansiyel gÃ¶rdÃ¼ÄŸÃ¼n Ã¼rÃ¼nleri yanÄ±na #potansiyel etiketiyle beraber yaz asaÄŸÄ±daki kurallara gÃ¶re.
    GÃ–REV: AÅŸaÄŸÄ±daki '{category}' verilerini analiz et.
    Kolon Ä°simleri (SÄ±rayla): [{column_names}]
    
    1. Sadece **Metro HORECA mÃ¼ÅŸterilerinin (restoran, kafe vb.) menÃ¼sÃ¼ne veya operasyonuna DEVRÄ°M YARATACAK** ve **yeni trend sinyali** taÅŸÄ±yan Ã¼rÃ¼nleri seÃ§.
    2. Ã‡Ã¶pleri kesinlikle at. **Uzun Ã¼rÃ¼n ismini, Ã¼rÃ¼nÃ¼n temel niteliÄŸi belli olacak ÅŸekilde KISALT.**
    3. Her Ã¼rÃ¼n iÃ§in **Potansiyel Skoru** (0-100) ver. Bu skor, Ã¼rÃ¼nÃ¼n *piyasada trend olma hÄ±zÄ±* ve *HORECA sektÃ¶rÃ¼ne katacaÄŸÄ± yenilik deÄŸeri* baz alÄ±narak belirlenmelidir.
    4. Ã‡IKTI JSON'unda **gÃ¶nderilen ham verinin ilk sÃ¼tunundaki bilgiyi** "kaynak_dosya" alanÄ±na aktar.
    5. JSON dÃ¶ndÃ¼r. LÃ¼tfen uzun analiz veya aÃ§Ä±klama YAPMA.

    VERÄ° (Kolon Ä°simleri hariÃ§tir, yukarÄ±daki listeye bakÄ±nÄ±z):
    {data_chunk}

    Ã‡IKTI: [{{ 
      "kaynak_dosya": "Ham verinin ilk sÃ¼tunundaki deÄŸer,markasÄ± veya benzersiz kimliÄŸi.",
      "urun_adi": "ÃœrÃ¼n AdÄ± (Mutlaka KISALTILMIÅ ama Ã¼rÃ¼n belirlenebilir olacak ÅŸekilde.)", 
      "fiyat": "sayÄ±sal deÄŸer olarak fiyat ve para birimi(varsa). Yoksa "-" iÅŸareti.", 
      "potansiyel_skoru": "0-100 arasÄ± tamsayÄ± .Potansiyel Skorunu pazara ve kendi verilerine ve marketlere gÃ¶re belirle ve bana gerÃ§eÄŸe en yakÄ±n skoru ver.",
      "not": "KÄ±sa aÃ§Ä±klama/etiket (Ã–rn: Erken Trend Sinyali, Vegan Alternatif, Ä°ÅŸletme VerimliliÄŸi gibi Ã¼rÃ¼nle alakalÄ± kendi mantÄ±ÄŸÄ±nla Ã¼rettiÄŸin  3 kÄ±sa etiket oluÅŸtur Ã¼rÃ¼nle alakalÄ±.etiketleri # ile baÅŸlat.)"
    }}]
    """
    
    try:
        completion = client.chat.completions.create(
            extra_headers={"HTTP-Referer": "http://localhost", "X-Title": "ProScraper"},
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
        )
        
        resp = completion.choices[0].message.content
        if "```" in resp:
            resp = resp.split("```json")[-1].split("```")[0].strip()
            resp = resp.replace("```", "").strip()
            
        return json.loads(resp)
    
    except Exception as e:
        err = str(e)
        if "402" in err or "insufficient_quota" in err:
            print("\nâŒ HATA: Yetersiz Bakiye! LÃ¼tfen OpenRouter'a kredi yÃ¼kleyin.")
            sys.exit(1)
            
        if retry < 3:
            print(f"      âš ï¸ GeÃ§ici Hata. Tekrar deneniyor... ({retry+1})")
            time.sleep(2)
            # Kolon isimlerini tekrar geÃ§erek yeniden dene
            return analyze_paid_fast(data_chunk, category, df_columns, retry + 1)
        return []

# --- ANA DÃ–NGÃœ ---
def process_files():
    # --- DÄ°NAMÄ°K YOL AYARLAMASI ---
    # Kodun Yeri: .../scraper/ai_filter/preprocessed_data/preprocessed_ai.py
    # Verinin Yeri: .../scraper/ai_filter/Raw_data
    # Bu yÃ¼zden BASE_DIR.parent (ai_filter) -> Raw_data yapÄ±yoruz.
    
    raw_data_dir = BASE_DIR.parent / "Raw_data"
    
    # Ã‡Ä±ktÄ± klasÃ¶rÃ¼ (data)
    output_dir = BASE_DIR / "data"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    target_files = ["Rival.csv", "online_shopping.csv", "social_media.csv"]
    
    print(f"ğŸ“‚ Okunacak KlasÃ¶r: {raw_data_dir}")
    print(f"ğŸ’ Model: {MODEL_NAME} (HAZIR)")
    print("------------------------------------------------")

    if not raw_data_dir.exists():
        print(f"âŒ HATA: Raw data klasÃ¶rÃ¼ bulunamadÄ±: {raw_data_dir}")
        return

    for filename in target_files:
        if not (raw_data_dir / filename).exists():
            print(f"âš ï¸ Dosya bulunamadÄ±, atlanÄ±yor: {filename}")
            continue

        print(f"\nğŸš€ {filename} Ä°ÅLENÄ°YOR...")
        
        try:
            df = pd.read_csv(raw_data_dir / filename, dtype=str, low_memory=False).fillna("")
        except Exception as e:
            print(f"âŒ Okuma hatasÄ± ({filename}): {e}")
            continue
        
        df_clean = clean_data(df)
        total_rows = len(df_clean)
        
        file_key = filename.split('.')[0]
        start_index = get_last_index(file_key)
        
        if start_index >= total_rows:
            print(f"   âœ… Zaten bitmiÅŸ.")
            continue
        elif start_index > 0:
            print(f"   â© {start_index}. satÄ±rdan devam.")

        # Kolon isimlerini bir kere al (AI'ya gÃ¶ndermek iÃ§in)
        df_columns = df_clean.columns.tolist()

        for i in range(start_index, total_rows, BATCH_SIZE):
            batch = df_clean.iloc[i : i + BATCH_SIZE]
            
            # KRÄ°TÄ°K: header=False ile ilk satÄ±r (kolon isimleri) gÃ¶nderilmez
            batch_str = batch.to_string(header=False, index=False) 
            
            print(f"   â³ Ä°ÅŸleniyor: {i} - {min(i+BATCH_SIZE, total_rows)} (Toplam: {total_rows})")
            
            # Kolon isimlerini analyze_paid_fast fonksiyonuna yolla
            results = analyze_paid_fast(batch_str, file_key, df_columns)
            
            if results:
                append_to_csv(results, file_key)
                print(f"      ğŸ’¾ {len(results)} veri EKLENDÄ°.")
            else:
                print("      âŒ Veri yok.")

            save_progress(file_key, i + BATCH_SIZE)
            time.sleep(WAIT_TIME)

        print(f"ğŸ‰ {filename} TAMAMLANDI!")

if __name__ == "__main__":
    process_files()