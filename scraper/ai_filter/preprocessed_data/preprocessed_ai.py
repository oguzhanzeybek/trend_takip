import os
import json
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
from pathlib import Path
import time
import sys
import warnings

# Gereksiz uyarÄ±larÄ± sustur
warnings.filterwarnings("ignore")

# --- YENÄ° EKLENTÄ°: ARAMA KÃœTÃœPHANESÄ° ---
try:
    from duckduckgo_search import DDGS
except ImportError:
    try:
        from ddgs import DDGS
    except ImportError:
        print("âŒ HATA: 'duckduckgo-search' kÃ¼tÃ¼phanesi eksik.")
        print("ğŸ‘‰ Ã‡Ã¶zÃ¼m: pip install duckduckgo-search")
        sys.exit(1)

BASE_DIR = Path(__file__).resolve().parent

env_path = None
search_dirs = [BASE_DIR] + list(BASE_DIR.parents)[:3]
for d in search_dirs:
    if (d / '.env').exists():
        env_path = d / '.env'
        load_dotenv(dotenv_path=env_path)
        break

api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENROUTER_KEY")

if not api_key:
    print("âŒ HATA: OPENROUTER_API_KEY veya OPENROUTER_KEY bulunamadÄ±! .env dosyasÄ±nÄ± kontrol et.")
    sys.exit(1)

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=api_key, 
)

MODEL_NAME = "openai/gpt-4o-mini"
BATCH_SIZE = 5 
WAIT_TIME = 1 

def truncate_text(text, max_chars=1000):
    """Token maliyetini dÃ¼ÅŸÃ¼rmek iÃ§in metni kÄ±saltÄ±r."""
    if len(text) > max_chars:
        return text[:max_chars] + "..."
    return text

def clean_data(df):
    """
    TEMÄ°ZLÄ°K VE KIRPMA
    """
    initial_len = len(df)
    print(f"   ğŸ§¹ Ã–n temizlik... (GiriÅŸ: {initial_len})")
    
    # --- KRÄ°TÄ°K EKLEME: Orijinal Index'i Kaybetmemek Ä°Ã§in SÃ¼tuna Ã‡eviriyoruz ---
    # Bu sayede drop_duplicates yapÄ±lsa bile orijinal sÄ±ra numarasÄ± "original_index" sÃ¼tununda kalÄ±r.
    df['original_index'] = df.index 
    
    df = df.dropna(how='all').drop_duplicates(subset=df.columns.difference(['original_index']))
    
    # Kolon isimlerini temizle (boÅŸluklarÄ± at)
    df.columns = df.columns.str.strip()
    
    df_temp = df.copy()
    
    # Metin kÄ±saltma iÅŸlemi (original_index hariÃ§)
    cols_to_process = [c for c in df_temp.columns if c != 'original_index']
    if cols_to_process:
        df_temp[cols_to_process] = df_temp[cols_to_process].astype(str).apply(
            lambda col: col.apply(lambda x: truncate_text(x, 1000))
        )
    
    print(f"   âœ¨ Veri HazÄ±r (AI Elemesi iÃ§in): {len(df_temp)} satÄ±r")
    return df_temp 

def get_progress_file_path(filename):
    data_dir = BASE_DIR / "data"
    data_dir.mkdir(parents=True, exist_ok=True)
    return data_dir / f"{filename}_progress.txt"

def get_last_index(filename):
    p_file = get_progress_file_path(filename)
    if p_file.exists():
        with open(p_file, "r") as f:
            try: return int(f.read().strip())
            except: return 0
    return 0

def save_progress(filename, index):
    with open(get_progress_file_path(filename), "w") as f:
        f.write(str(index))

def append_to_csv(data, filename):
    output_path = BASE_DIR / "data" / f"filtered_{filename}.csv"
    df = pd.DataFrame(data)
    
    cols = ['rank', 'kaynak_dosya', 'urun_adi', 'fiyat', 'potansiyel_skoru', 'link', 'not', 'aciklama']
    
    available_cols = [c for c in cols if c in df.columns]
    df = df[available_cols]

    if not output_path.exists():
        df.to_csv(output_path, index=False, encoding='utf-8-sig', mode='w')
    else:
        df.to_csv(output_path, index=False, encoding='utf-8-sig', mode='a', header=False)

# --- CANLI ARAMA FONKSÄ°YONU ---
def search_live_context(keyword):
    if not keyword or len(keyword) < 2: return "Veri yok."
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(f"{keyword} fiyatÄ± ne kadar", region='tr-tr', safesearch='off', max_results=2))
            if not results: return "Ä°nternette gÃ¼ncel bilgi bulunamadÄ±."
            context = " | ".join([f"{r['title']}: {r['body']}" for r in results])
            return context[:800] 
    except:
        return "Arama yapÄ±lamadÄ±."

# --- GÃœNCELLENEN ANALÄ°Z FONKSÄ°YONU ---
def analyze_paid_fast(df_chunk, file_key, df_columns, retry=0):
    results_list = []
    
    cols_lower = {c.lower(): c for c in df_chunk.columns}
    
    # Kritik SÃ¼tunlarÄ± Belirle
    col_kaynak = cols_lower.get("kaynak") or cols_lower.get("source")
    col_fiyat = cols_lower.get("fiyat") or cols_lower.get("price")
    col_link = cols_lower.get("link") or cols_lower.get("url")

    # ÃœrÃ¼n SÃ¼tununu AkÄ±llÄ± Bulma
    possible_names = ["urun", "urun_adi", "Ã¼rÃ¼n adÄ±", "Ã¼rÃ¼n baÅŸlÄ±ÄŸÄ±", "product_name", "product", "name", "title", "trend"]
    col_urun = None
    for name in possible_names:
        if name in cols_lower:
            col_urun = cols_lower[name]
            break

    # SATIR SATIR Ä°ÅLE
    for _, row in df_chunk.iterrows():
        
        # --- KRÄ°TÄ°K DÃœZELTME: ORÄ°JÄ°NAL INDEX'Ä° SÃœTUNDAN AL ---
        # ArtÄ±k loop indexini deÄŸil, clean_data'da sakladÄ±ÄŸÄ±mÄ±z 'original_index' deÄŸerini kullanÄ±yoruz.
        real_original_rank = row.get('original_index', 0) 
        
        original_source = str(row[col_kaynak]) if col_kaynak else file_key 
        original_link = str(row[col_link]) if col_link else ""
        original_price = str(row[col_fiyat]) if col_fiyat else ""
        
        product_name = ""
        if col_urun:
            product_name = str(row[col_urun])
        else:
            vals = [str(x) for x in row.values if x != real_original_rank and not str(x).isdigit() and not str(x).startswith("http")]
            product_name = max(vals, key=len) if vals else "Bilinmeyen ÃœrÃ¼n"

        # Ä°sim KÄ±saltma
        short_name = product_name
        if len(product_name) > 60:
            short_name = product_name[:57] + "..."

        # CanlÄ± Arama
        print(f"      ğŸ” Analiz Ediliyor (Orj Index: {real_original_rank}): {short_name}...")
        search_context = search_live_context(product_name) 
        time.sleep(1)

        # --- GÃœNCELLENMÄ°Å, DAHA SERT SKORLAMA PROMPT'U ---
        prompt = f"""
        Sen acÄ±masÄ±z, gerÃ§ekÃ§i bir tÃ¼ccar ve pazar analistisin. 
        Hayal satma, gerÃ§ek verilere ve ticari mantÄ±ÄŸa odaklan.

        ÃœRÃœN: {product_name}
        MEVCUT FÄ°YAT (Varsa): {original_price}
        Ä°NTERNET VERÄ°SÄ°: {search_context}
        
        GÃ–REV:
        Bu Ã¼rÃ¼n al-sat (arbitraj) yapmak veya stoklamak iÃ§in KARLI MI?
        
        PUANLAMA ALGORÄ°TMASI (0-100):
        - 0-50: Ã‡Ã¶p. Her yerde var, kÃ¢r marjÄ± yok, modasÄ± geÃ§miÅŸ veya kimsenin almayacaÄŸÄ± Ã¼rÃ¼n.
        - 51-74: Riskli. Belki satar ama uÄŸraÅŸmaya deÄŸmez.
        - 75-89: Ä°yi FÄ±rsat. Talep var, fiyat rekabetÃ§i olabilir.
        - 90-100: AltÄ±n Yumurtlayan Tavuk. Kesinlikle listeye girmeli.

        KURALLAR:
        1. ÃœrÃ¼n Ã§ok yaygÄ±nsa (market raf Ã¼rÃ¼nÃ¼ vb.) dÃ¼ÅŸÃ¼k puan ver.
        2. Fiyat bilgisi yoksa internet verisine bak, tahmin et.
        
        Ã‡IKTI FORMATI (Sadece JSON):
        {{ 
            "aciklama": "Neden mantÄ±klÄ± veya deÄŸil? (Net, kÄ±sa, ticari yorum)", 
            "not": "#Ä°lgiliHashtagler", 
            "potansiyel_skoru": (SayÄ±sal Puan) 
        }}
        """
        
        try:
            completion = client.chat.completions.create(
                extra_headers={"HTTP-Referer": "http://localhost"},
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
            )
            resp = completion.choices[0].message.content
            if "```" in resp: resp = resp.split("```json")[-1].split("```")[0].strip()
            ai_data = json.loads(resp)
            
            score = int(ai_data.get("potansiyel_skoru", 0))

            # --- EÅÄ°K DEÄERÄ° ---
            if score < 65:
                print(f"      ğŸ—‘ï¸  DÃ¼ÅŸÃ¼k Puan ({score}): {short_name} -> ELENDÄ°.")
                continue 

            print(f"      âœ…  YÃ¼ksek Puan ({score}): {short_name} -> EKLENÄ°YOR.")
            
            final_obj = {
                "rank": real_original_rank,             # --- DÃœZELTÄ°LDÄ°: Kesinlikle orijinal index ---
                "kaynak_dosya": original_source,
                "urun_adi": short_name,
                "fiyat": original_price,
                "potansiyel_skoru": score,
                "link": original_link,
                "not": ai_data.get("not", ""),
                "aciklama": ai_data.get("aciklama", "AÃ§Ä±klama yok.")
            }
            results_list.append(final_obj)
            
        except Exception as e:
            print(f"      âš ï¸ AI HatasÄ± (SatÄ±r atlandÄ±): {e}")
            continue

    return results_list

def process_files():
    
    raw_data_dir = BASE_DIR.parent / "Raw_data"
    output_dir = BASE_DIR / "data"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    target_files = ["Rival.csv", "online_shopping.csv", "social_media.csv"]
    
    print(f"ğŸ“‚ Okunacak KlasÃ¶r: {raw_data_dir}")
    print(f"ğŸ’ Model: {MODEL_NAME} (HAZIR)")
    print(f"ğŸŒ Mod: Orijinal Index KorumalÄ± + Sert Filtre")
    print("------------------------------------------------")

    if not raw_data_dir.exists():
        print(f"âŒ HATA: Raw data klasÃ¶rÃ¼ bulunamadÄ±: {raw_data_dir}")
        return

    for filename in target_files:
        if not (raw_data_dir / filename).exists():
            print(f"âš ï¸ Dosya bulunamadÄ±, atlanÄ±yor: {filename}")
            continue

        print(f"\nğŸš€ {filename} Ä°ÅLENÄ°YOR...")
        
        try:
            # SÃ¼tun isimleri ne olursa olsun okur
            df = pd.read_csv(raw_data_dir / filename, dtype=str, low_memory=False).fillna("")
        except Exception as e:
            print(f"âŒ Okuma hatasÄ± ({filename}): {e}")
            continue
        
        df_clean = clean_data(df)
        total_rows = len(df_clean)
        
        file_key = filename.split('.')[0]
        start_index = get_last_index(file_key) # Buradaki index artÄ±k iÅŸlenen satÄ±r sayÄ±sÄ±dÄ±r
        
        if start_index >= total_rows:
            print(f"   âœ… Zaten bitmiÅŸ.")
            continue
        elif start_index > 0:
            print(f"   â© {start_index}. sÄ±radaki veriden devam ediliyor.")

        df_columns = df_clean.columns.tolist()

        for i in range(start_index, total_rows, BATCH_SIZE):
            batch_df = df_clean.iloc[i : i + BATCH_SIZE]
            
            print(f"   â³ Ä°ÅŸleniyor (Batch): {i} - {min(i+BATCH_SIZE, total_rows)}")
            
            results = analyze_paid_fast(batch_df, file_key, df_columns)
            
            if results:
                append_to_csv(results, file_key)
                print(f"      ğŸ’¾ {len(results)} FÄ±rsat ÃœrÃ¼nÃ¼ EKLENDÄ°.")
            else:
                print("      âŒ Bu partide uygun Ã¼rÃ¼n Ã§Ä±kmadÄ±.")

            save_progress(file_key, i + BATCH_SIZE)

        print(f"ğŸ‰ {filename} TAMAMLANDI!")

if __name__ == "__main__":
    process_files()
    
    print("\nğŸ§¹ TÃ¼m iÅŸlemler bitti, geÃ§ici progress dosyalarÄ± temizleniyor...")
    
    files_to_clean = ["Rival", "online_shopping", "social_media"]
    
    for file_key in files_to_clean:
        progress_path = get_progress_file_path(file_key)
        
        if progress_path.exists():
            try:
                progress_path.unlink()
                print(f"   ğŸ—‘ï¸  SÄ°LÄ°NDÄ°: {progress_path.name}")
            except Exception as e:
                print(f"   âš ï¸ SÄ°LÄ°NEMEDÄ°: {progress_path.name} -> {e}")
        else:
            print(f"   â„¹ï¸  Zaten yok: {progress_path.name}")

    print("ğŸ PROGRAM SONLANDI.")