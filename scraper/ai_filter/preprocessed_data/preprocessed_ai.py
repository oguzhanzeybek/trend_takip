import os
import json
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
from pathlib import Path
import time
import sys
import warnings
import random
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# Gereksiz uyarÄ±larÄ± sustur
warnings.filterwarnings("ignore")

# --- KÃœTÃœPHANE KONTROLÃœ ---
try:
    from ddgs import DDGS
except ImportError:
    try:
        from duckduckgo_search import DDGS
    except ImportError:
        print("âŒ HATA: 'duckduckgo-search' veya 'ddgs' kÃ¼tÃ¼phanesi eksik.")
        print("ğŸ‘‰ Ã‡Ã¶zÃ¼m: pip install ddgs")
        sys.exit(1)

BASE_DIR = Path(__file__).resolve().parent

# .env dosyasÄ±nÄ± bulma
env_path = None
search_dirs = [BASE_DIR] + list(BASE_DIR.parents)[:3]
for d in search_dirs:
    if (d / '.env').exists():
        env_path = d / '.env'
        load_dotenv(dotenv_path=env_path)
        break

api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENROUTER_KEY")

if not api_key:
    print("âŒ HATA: API Key bulunamadÄ±! .env dosyasÄ±nÄ± kontrol et.")
    sys.exit(1)

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=api_key, 
)

MODEL_NAME = "openai/gpt-4o-mini"
BATCH_SIZE = 5     
MAX_WORKERS = 3    

def truncate_text(text, max_chars=1000):
    if len(text) > max_chars: return text[:max_chars] + "..."
    return text

def clean_data(df):
    initial_len = len(df)
    print(f"   ğŸ§¹ Veri Temizleniyor... (GiriÅŸ: {initial_len})")
    
    # Rank'i dosya okunduÄŸu andaki satÄ±r sÄ±rasÄ±na gÃ¶re sabitliyoruz.
    df['original_index'] = range(2, len(df) + 2)
    
    df = df.dropna(how='all')
    df.columns = df.columns.str.strip()
    
    df_temp = df.copy()
    cols_to_process = [c for c in df_temp.columns if c != 'original_index']
    if cols_to_process:
        df_temp[cols_to_process] = df_temp[cols_to_process].astype(str).apply(
            lambda col: col.apply(lambda x: truncate_text(x, 1000))
        )
    print(f"   âœ¨ Veri HazÄ±r: {len(df_temp)} satÄ±r")
    return df_temp 

def get_progress_file_path(filename):
    data_dir = BASE_DIR / "data"
    data_dir.mkdir(parents=True, exist_ok=True)
    return data_dir / f"{filename}_progress.txt"

def get_last_index(filename):
    p_file = get_progress_file_path(filename)
    if p_file.exists():
        with open(p_file, "r") as f:
            try: return int(f.read().strip())
            except: return 0
    return 0

def save_progress(filename, index):
    with open(get_progress_file_path(filename), "w") as f:
        f.write(str(index))

def append_to_csv(data, filename):
    output_path = BASE_DIR / "data" / f"filtered_{filename}.csv"
    df = pd.DataFrame(data)
    
    cols = [
        'rank', 
        'kaynak_dosya', 
        'urun_adi', 
        'fiyat', 
        'potansiyel_skoru', 
        'trend_durumu',
        'hedef_kitle',
        'risk_analizi',
        'rakip_durumu',
        'pazarlama_fikri',
        'video_ozeti',      
        'manipulasyon_riski', 
        'kalite_puani',     
        'hype_puani',       
        'fiyat_puani',      
        'urun_resmi', 
        'link', 
        'aciklama'          
    ]
    
    for c in cols:
        if c not in df.columns: df[c] = ""
    df = df[cols]

    if not output_path.exists():
        df.to_csv(output_path, index=False, encoding='utf-8-sig', mode='w')
    else:
        df.to_csv(output_path, index=False, encoding='utf-8-sig', mode='a', header=False)

# --- SEARCH GOD MODE ---
def search_god_mode(keyword, rank, retries=0):
    if not keyword or len(keyword) < 2: return None, "Veri yok."
    
    context_accumulator = []
    first_image_url = ""

    is_hashtag = keyword.startswith("#") or "gÃ¼ndem" in keyword.lower() or "olayÄ±" in keyword.lower()

    if is_hashtag:
        queries = [
            f"{keyword} olayÄ± nedir neden gÃ¼ndem oldu",
            f"{keyword} tepkiler twitter ekÅŸi",
            f"{keyword} son durum haberler"
        ]
    else:
        queries = [
            f"{keyword} kronik sorunlar iade nedenleri ÅŸikayet", 
            f"{keyword} alÄ±nÄ±r mÄ± fiyatÄ±na deÄŸer mi inceleme",   
            f"{keyword} vs rakipleri en iyi alternatifi",        
            f"{keyword} kimler kullanÄ±yor hedef kitle",          
            f"{keyword} fiyat geÃ§miÅŸi indirim trendi"            
        ]

    try:
        with DDGS() as ddgs:
            # A) GÃ–RSEL
            try:
                res_img = list(ddgs.images(keyword, region='tr-tr', safesearch='off', max_results=1))
                if res_img: first_image_url = res_img[0].get('image', '')
            except: pass

            # B) METÄ°N ARAMA
            for q in queries:
                try:
                    res = list(ddgs.text(q, region='tr-tr', safesearch='off', max_results=2))
                    if res:
                        chunk = " | ".join([f"KAYNAK: {r['title']} -> {r['body']}" for r in res])
                        context_accumulator.append(chunk)
                    time.sleep(random.uniform(0.5, 1.0)) 
                except: continue

            # C) VÄ°DEO ARAMA
            if not is_hashtag:
                try:
                    vid_query = f"{keyword} inceleme review test sakÄ±n almayÄ±n"
                    res_vid = list(ddgs.videos(vid_query, region='tr-tr', safesearch='off', max_results=3))
                    if res_vid:
                        vid_chunk = " | ".join([f"VIDEO: {r['title']} (AÃ§Ä±klama: {r['description']})" for r in res_vid])
                        context_accumulator.append(f"--- YOUTUBE VERÄ°LERÄ° ---\n{vid_chunk}")
                except: pass

        final_context = "\n\n".join(context_accumulator)[:10000] 
        
        if not final_context.strip():
            return first_image_url, "VERÄ° BULUNAMADI"

        return first_image_url, final_context
            
    except Exception as e:
        if retries < 2:
            time.sleep(2)
            return search_god_mode(keyword, rank, retries + 1)
        return "", "Arama HatasÄ±"

# --- TEK BÄ°R SATIRI Ä°ÅLEYEN FONKSÄ°YON ---
def process_single_row(row, file_key, cols_map):
    col_kaynak = cols_map['kaynak']
    col_fiyat = cols_map['fiyat']
    col_link = cols_map['link']
    col_urun = cols_map['urun']
    
    real_original_rank = str(row.get('original_index', 0))
    
    detected_source = file_key 
    if col_kaynak and str(row[col_kaynak]).strip().lower() not in ["nan", "", "none"]:
        detected_source = str(row[col_kaynak]).strip()

    original_link = str(row[col_link]) if col_link else ""
    original_price = str(row[col_fiyat]) if col_fiyat else "BelirtilmemiÅŸ"
    
    product_name = ""
    if col_urun: 
        val = str(row[col_urun]).strip()
        if not val.startswith("http") and len(val) > 1:
            product_name = val
    
    if not product_name:
        vals = [str(x) for x in row.values if x != real_original_rank and not str(x).isdigit() and not str(x).startswith("http") and str(x) != detected_source]
        product_name = max(vals, key=len) if vals else "Bilinmeyen ÃœrÃ¼n"

    short_name = product_name
    if len(product_name) > 60: short_name = product_name[:57] + "..."

    print(f"      âš¡ Analiz (Rank: {real_original_rank}): {short_name}...")
    
    image_url, search_context = search_god_mode(product_name, real_original_rank)
    
    # EÄER VERÄ° YOKSA DÄ°REKT ELE (AcÄ±masÄ±z Mod)
    if search_context == "VERÄ° BULUNAMADI":
        print(f"      ğŸ—‘ï¸  Veri Yok -> ELENDÄ° (Rank: {real_original_rank})")
        return None

    # --- ACIMASIZ PROMPT ---
    prompt = f"""
    GÃ–REV: Sen 'AcÄ±masÄ±z Bir TÃ¼ccar' ve 'Risk Analistisin'.
    ParanÄ± Ã§Ã¶pe atmaktan nefret edersin. Ã–nÃ¼ne gelen her Ã¼rÃ¼nÃ¼ Ã¶vme.
    Sadece %10'luk "Elmas DeÄŸerindeki" fÄ±rsatlarÄ± arÄ±yorsun.
    
    GÄ°RDÄ°LER:
    KONU: {product_name}
    FÄ°YAT: {original_price}
    MEVCUT_KAYNAK_BILGISI: {detected_source}
    Ä°STÄ°HBARAT (Web + Video + Haber): {search_context}

    --- KRÄ°TÄ°K DEÄERLENDÄ°RME ---
    1. ÃœrÃ¼n hakkÄ±nda internette ÅŸikayet var mÄ±?
    2. FiyatÄ± piyasaya gÃ¶re pahalÄ± mÄ±?
    3. Bu Ã¼rÃ¼n gerÃ§ekten trend mi yoksa sÃ¶nmÃ¼ÅŸ bir balon mu?
    
    EÄŸer Ã¼rÃ¼n vasatsa, sÄ±radansa veya riskliyse DÃœÅÃœK PUAN VER (40-50).
    Sadece "Vay canÄ±na, bu kesin satar" dediÄŸin Ã¼rÃ¼nlere 80+ ver.

    --- Ã‡IKTI FORMATI (JSON) ---
    0. "kaynak_dosya": '{detected_source}'.
    1. "aciklama": Patron iÃ§in KISA, NET ve OBJEKTÄ°F bir Ã¶zet (Max 3 cÃ¼mle). OlumsuzluklarÄ± gizleme.
    2. "trend_durumu": ÃœrÃ¼nÃ¼n yaÅŸam dÃ¶ngÃ¼sÃ¼ (DÃ¼ÅŸÃ¼ÅŸte/Stabil/YÃ¼kseliÅŸte).
    3. "hedef_kitle": Kime satacaÄŸÄ±z?
    4. "risk_analizi": Neden batabiliriz? (Ä°ade, bozulma, modasÄ± geÃ§me).
    5. "rakip_durumu": Rakipler daha mÄ± ucuz?
    6. "pazarlama_fikri": SatÄ±ÅŸ sloganÄ±.
    7. "video_ozeti": Videolardaki genel hava (Olumlu/Olumsuz).
    8. "manipulasyon_riski": Yorumlar bot mu?
    9. "kalite_puani": (0-100)
    10. "hype_puani": (0-100)
    11. "fiyat_puani": (0-100)
    12. "potansiyel_skoru": (0-100 ArasÄ± Kesin Puan. 70 ALTI BAÅARISIZDIR).

    SADECE JSON DÃ–NDÃœR.
    """
    
    try:
        completion = client.chat.completions.create(
            extra_headers={"HTTP-Referer": "http://localhost"},
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,  # Daha tutarlÄ± ve ciddi olmasÄ± iÃ§in dÃ¼ÅŸÃ¼rdÃ¼m
            max_tokens=2000
        )
        resp = completion.choices[0].message.content
        if "```" in resp: resp = resp.split("```json")[-1].split("```")[0].strip()
        ai_data = json.loads(resp)
        
        score = int(ai_data.get("potansiyel_skoru", 0))

        # --- YENÄ° BARAJ: 70 ---
        # Eskiden 50 idi, ÅŸimdi 70. Vasat Ã¼rÃ¼nler elenir.
        if score < 70:
            print(f"      ğŸ“‰ Yetersiz ({score}) (Rank: {real_original_rank}) -> ELENDÄ°")
            return None

        print(f"      ğŸ’ ONAYLANDI (Skor: {score}) (Rank: {real_original_rank}) -> EKLENDÄ°")
        
        final_source = ai_data.get("kaynak_dosya", detected_source)
        if not final_source: final_source = detected_source

        return {
            "rank": real_original_rank,     
            "kaynak_dosya": final_source, 
            "urun_adi": short_name,
            "fiyat": original_price,
            "potansiyel_skoru": score,
            "kalite_puani": ai_data.get("kalite_puani", 0),
            "hype_puani": ai_data.get("hype_puani", 0),
            "fiyat_puani": ai_data.get("fiyat_puani", 0),
            "trend_durumu": ai_data.get("trend_durumu", "-"),
            "hedef_kitle": ai_data.get("hedef_kitle", "-"),
            "risk_analizi": ai_data.get("risk_analizi", "-"),
            "rakip_durumu": ai_data.get("rakip_durumu", "-"),
            "pazarlama_fikri": ai_data.get("pazarlama_fikri", "-"),
            "video_ozeti": ai_data.get("video_ozeti", "-"),
            "manipulasyon_riski": ai_data.get("manipulasyon_riski", "-"),
            "urun_resmi": image_url,
            "link": original_link,
            "aciklama": ai_data.get("aciklama", "Detay yok.")
        }
        
    except Exception as e:
        print(f"      âš ï¸ Hata (Rank: {real_original_rank}): {e}")
        return None

# --- PARALEL Ä°ÅLEM YÃ–NETÄ°CÄ°SÄ° ---
def analyze_god_mode_ai(df_chunk, file_key, df_columns):
    results_list = []
    
    cols_lower = {c.lower(): c for c in df_chunk.columns}
    cols_map = {
        'kaynak': cols_lower.get("kaynak") or cols_lower.get("source"),
        'fiyat': cols_lower.get("fiyat") or cols_lower.get("price"),
        'link': cols_lower.get("link") or cols_lower.get("url"),
        'urun': None
    }
    
    possible_names = ["urun", "urun_adi", "Ã¼rÃ¼n adÄ±", "Ã¼rÃ¼n baÅŸlÄ±ÄŸÄ±", "product_name", "title", "trend", "hashtag"]
    for name in possible_names:
        if name in cols_lower:
            cols_map['urun'] = cols_lower[name]
            break

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = []
        for _, row in df_chunk.iterrows():
            futures.append(executor.submit(process_single_row, row, file_key, cols_map))
        
        for future in as_completed(futures):
            res = future.result()
            if res:
                results_list.append(res)
    
    results_list.sort(key=lambda x: int(x['rank']) if str(x['rank']).isdigit() else 999999)

    return results_list

def process_files():
    raw_data_dir = BASE_DIR.parent / "Raw_data"
    output_dir = BASE_DIR / "data"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    target_files = ["Rival.csv", "online_shopping.csv", "social_media.csv"]
    
    print(f"ğŸ“‚ VeritabanÄ±: {raw_data_dir}")
    print(f"ğŸ‘‘ MOD: ACIMASIZ & HIZLI (Baraj: 70)")
    print("------------------------------------------------")

    if not raw_data_dir.exists():
        print("âŒ KlasÃ¶r yok!")
        return

    for filename in target_files:
        if not (raw_data_dir / filename).exists(): continue

        print(f"\nğŸš€ {filename} ANALÄ°Z BAÅLIYOR...")
        
        try:
            df = pd.read_csv(
                raw_data_dir / filename, 
                dtype=str, 
                low_memory=False, 
                index_col=False
            ).fillna("")
        except: continue
        
        df_clean = clean_data(df)
        total_rows = len(df_clean)
        file_key = filename.split('.')[0]
        start_index = get_last_index(file_key)
        
        if start_index >= total_rows:
            print("   âœ… TamamlanmÄ±ÅŸ.")
            continue

        df_columns = df_clean.columns.tolist()

        for i in range(start_index, total_rows, BATCH_SIZE):
            batch_df = df_clean.iloc[i : i + BATCH_SIZE]
            print(f"   â³ Batch Ä°ÅŸleniyor: {i} - {min(i+BATCH_SIZE, total_rows)}")
            
            results = analyze_god_mode_ai(batch_df, file_key, df_columns)
            
            if results:
                append_to_csv(results, file_key)
                print(f"      ğŸ’¾ {len(results)} Adet FÄ±rsat Kaydedildi.")
            else:
                print(f"      ğŸ—‘ï¸  Bu gruptan hiÃ§biri barajÄ± geÃ§emedi.")

            save_progress(file_key, i + BATCH_SIZE)

        print(f"ğŸ‰ {filename} Bitti!")

if __name__ == "__main__":
    process_files()
    for fk in ["Rival", "online_shopping", "social_media"]:
        pp = get_progress_file_path(fk)
        if pp.exists(): pp.unlink()
    print("ğŸ YÃ–NETÄ°M RAPORU TAMAMLANDI.")